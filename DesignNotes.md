
**CASSANDRA**
he problem with relational database scale up, high available and sharding.

Relational database: host stand-alone, hundreads of concurrent users, ACID guarantees.
replication lag (fairly, which system does not have rep lag).
Relational database for handle load.
Can be denormalize. This redundancy increases. The original ACID? Hmmm.
Sharding separates the data. Joins, aggregation, the problems that the original RDBMS can solve are not available.
Even if you use the first layer index to shard, if you want to query other indexes. It is necessary to hit all the shards, and you must also use external aggregation.
The addition and subtraction of sharding requires manual movement of data.

High availability (using amazon's integrated products can probably be avoided?) implementation has a lot of operational overhead. (multi-DC)

cass (referred to as Cassandra's short name below) is:
* Abandon the difficult to achieve consistency.
* built in sharding.
* No master-slave. This should be a mountain.
* distributed.
* Using some kind of denormalization optimization, the purpose is to use only one shard in the query. External aggregation is of course not used anymore.

Some features:
high performance,
High availability
Linear scalability. Netflix seems to be really verifying this. From 50 nodes to 300 nodes.
Low latency.
There is no single point of attachment, and it is quite confused on multiple data centers.
No master-slave. Peer-to-peer. There is no leader election.
A lot of operational tools. OPS is convenient.
However, it is obvious that RDBMS cannot be completely replaced. And some (or many) features can't be copied. That is to say, you can't do ACID.
AP system. The delay between data centers makes it difficult to guarantee that the guarantee is abandoned. But not completely give up, each query can choose three levels of consistency level: ONE, QUORUM, ALL means query or write consistency. How many ACKs you need to get is considered successful. This setting will affect the consistency but will also affect: read and write speed. Data security, availability.

Famous hash ring
The partition key (consistent hashing) determines which shard to go to.
Replicated to X nodes.
There is no need for centralized zookeeper. Point-to-point gossip.

Replication is automatic. (The re-sharding of the RDBMS mentioned above is directly manual comparison). If a node hangs, the new write data is stored as a hint for the new alternate node to digest.
Because the common mode is multi-data center, there are two replication parameters: one is the number of replication inside the center. The other is the number of replications between centers.
The replication factor parameter is for keyspace.

concept:
The keyspace is the peer of the schema or database.
Hint is a concept in gossip that is a single write operation. Gossip will be learned separately later.

assandra is a database optimized for writing.

concept:
Coordinator, no special nodes. Write the node that the request enters (looks like no specification, roundrobin). This node notifies the nodes responsible for recording the partition key.
Sstable is a file, which is immutable data for row data. Small sstables will be merged to determine which version is later based on the timestamp of the write operation. Therefore the previous version will be overwritten. Sstable is the extraction required for backup.

Write operation
Write to commitlog first, then merge into memtable. The memtable discontinuity is written to the hard disk: sstable.
Each write operation has a timestamp. There will be a problem if the conflict wins which version.
The delete operation will be written to the tombstone record, also with timestamp.

The read operation
is also the first encounter with the coordinator, the coordinator to find the node that records the partition key and query.
The query may trigger the sstable merge (because it is possible that the same row will have several versions of sstables)
when nodes return. If the query's consistency level scheme is ALL, then all nodes on the line will reach a consensus. If it is not ALL, it will return the result, and the found conflict will be repaired and repaired in the background. (eventual consistency). There is also a probability of read_repair_chance, not every conflict will result in a repair that is executed immediately. Deferred repair seems to be more optimized.

There is no centralized zookeeper. Do not understand where the persistent ring state is obtained?
Each node scatters 256 points on the
ring, and the state jump to NORMAL indicates that the cass node is online

.
Nodetool status
Cql

Datatype:
        Text - think varchar, unbounded
        Int - signed 32 bit
        Timestamp (seconds level)
        Uuid (generated by uuic())
        Timeuuid (new concept, generated by now(), sortable uuid by time)

Cql
        Create
        Use
        Describe
        Select
        Copy (easy import/export data from/to csv) There is also a bulkload

The table is basically the same as in the RDBMS.
Optimize for application. The query mentioned above upstairs hopes to use only one shard, not a lot of shards.
Partition by application. But I still don't understand how the secondary index prevents cross first index shards.

oncept partitioner. The given key in the algorithm title, the hash part.


PRIMARY KEY guarantees uniqueness

PRIMARY KEY((A1, A2), B, C, D)
A1 and A2 are sharding keys.
The order below is B, C, D, which is the multiple keys of the general database.

The clustering columns guarantee that the internal index of ordering

WITH CLUSTERING ORDER BY (C DESC, B ASC)
will be C and B first.

When query is qualified = must be placed before the < > substatement. The last time cass is done is O(N) linear read after the O(LogN) range query is finished. This is the root cause.

When is cql prompting you to ALLOW FILTERING? It is very likely that our model is wrong.

PS for each individual node: 3000 to 5000 per core

without leader-follower. The failover people are also too slow. Awesome, cass.

There is no problem in the middle of the ring. Because each node on the consistent hash will have 256 sprinkles. The coordinator finds all the nodes for a sharding key, as long as one can reach it.

The repair of the background (triggered by the chance) will complete the final consistency.

Gossip is a diffusion protocol. One pass three, three pass nine exponential one-to-one pass the truth. I don't know when the implementation is how each participant decides to stop. Does everyone ask everyone in the village if you know who is going to get engaged? I know that. That is O(N^2). There should be better. No matter what.

The feature of gossip is normalization. One round per second. The small amount of data is negligible in the entire bandwidth.

The seed node is not a special node. But it seems that some of you are spreading to A, A said it already knows. Then A's seed node index is +1. In the future, you will be more eager to find A. What is
downed node

gossip? The metadata of the cluster. Heartbeat state (HB, version). Application state (in which DC, which Rack, load, state normal online or offline?).
Three-way handshake of TCP. The SYN, ACK, and ACK2

tools have this command:

nodetool gossipinfo

pstairs I have a question, since there is no central information source, then who knows the range on the consistent hashing ring? The network snitch is responsible for the normal propagation of the interval that each node is responsible for.

When the coordinator processes a read/write request, it sends it to the node responsible for the hash according to what it knows (via snitch gossip).

When replication=3, it is which node is found on the ring and two of it. Replication

between different (logical or physical) DCs:

CREATE KEYSPACE A WITH REPLICATION = { 'class': 'NetworkTopologyStrategy', 'dc-europe':2, 'dc-america':3, 'dc-china ': 5}; The

coordinator not only hits the request to the node in the DC, but also hits the node on the other DC. Very powerful

Read write repair compaction
This building:	  0%  ( 0 )	
  	0%  ( 0 )  
Global:	ðŸ‘    99%  ( 452 )	
  	0%  ( 4 )     ðŸ‘Ž

Hinted Handoff when the node is broken, the
coordinator is responsible for temporarily storing the write request. Up to several hours (default 3). If the broken node is getting better again during this time. Then these temporary write requests are wholesaled.

The Read repair
coordinator sends the read request to the relevant node. Want data.
Suppose three nodes are responsible for the request. One of the nodes returns the data and the other two return the checksum.
The coordinator is responsible for checking for consistency. If you agree, say it.
If they are inconsistent, I said above. There is a certain probability that needs to be fixed. Either the coordinator will ask for data from the other two nodes. Send the data that each row needs to be repaired back to the nodes that need to be repaired. Consistency is achieved. Either the background is fixed. All are the same program. Obviously online and so on are more delayed.

Write path
commit log is always append only. Memtable will be the same as the final sstable storage. Includes clustering order. Ss is the abbreviation of sorted stream.
The merge between multiple sstables is the principle of merge sort.

Read path
sstable has index, which is included in a sstable document, the relationship between partition key and byte offset.
The index of sstable is not in the document, it is too slow to read. Cass also saw an index index called partition summary, in memory. Binary search finds the bucket and then reads the real offset in the sstable file. Then read the real data.
The offset of the recently read records exists in the stand-alone key cache. Do you think this is not always invalidate?
Also use the bloom filter to determine whether the partition is in the sstable of this node.
The bloomfilter, partition summary, and key cache are only in memory.

Compaction
has a timestamp for each cell in each row to store when it is written. So when you merge, it is a process of comparing who uses it.
Delete is tombstone. The default lifetime of tombstone is 10 days. After 10, the compaction expires and is automatically erased in the compaction. Returns the occupied hard disk space.
Compact sstable is the process of for (each partition-pair in sstable1 and sstable2) compact partition-pair.
The compaction strategy knows that there is a concept to determine which two sstables to perform the compaction, so don't go into it

Active anti-entropy
This building:	  0%  ( 0 )	
  	0%  ( 0 )  
Global:	ðŸ‘    99%  ( 452 )	
  	0%  ( 4 )     ðŸ‘Ž
This post was last edited by 14417335 at 2019-4-14 05:05.


convergence resolution: active anti-entropy

If the network is disconnected frequently or a large number of deletion operations, the data's inconsistency will become higher. In order to control the state of chaos, an important means of Anti-entropy is to integrate. I haven't figured out how this integration is automatically controlled. But does not hinder understanding the details of the algorithm. Found: major compaction to repair a node. Major compaction is automatic or can be a nodetool manual trigger.

The famous merkle tree. Is a perfect binary tree. Each node is a hash (the hash of the left child, the hash of the right child). Except for the lowest level of the leaf. The value of leaf node is hash(data block)

[Java] plain text view copy code
?
                                        ROOT HASH = f(A, B)
                                   /                                       \
                   A = f(c, d)                                         B = f(e, f)
                   /          \                                /                  \
c=hash(partition1) d=hash(partition2)    e=hash(partition3)    f=hash(partition4)


The major compaction produces the merkle tree of all the partitions that it is responsible for (the column family does not currently understand). Give the tree to the node hosting the repair work (I don't know which one, no matter what).

The node hosting the repair work will require the replication node of each partition to also create the merkel tree (I guess it is based on the first tree, because the node and the node are responsible for the partition, if not updated according to the first tree The new merkel tree, then the tree and the tree are not the same). Then each tree is compared to each tree. After the difference is found, the related nodes will perform a more detailed data comparison, and the timestamp update will be written by the old version of the node. This process should be O(N). N is the number of all nodes.

This part really does not understand too much. However, learning the merkle tree is also a reward

Comment on the formula W+R>N:

R = number of nodes used for read operations
W = number of nodes used for write operations
N=Replication Factor. Not the number of nodes in the entire cluster.

W+R>N guarantees consistency.
A few examples:
W=1, R=N, 1+N > N
W=N, R=1, N+1 > N
W=Q, R=Q where Q=N / 2 + 1, 2*Q >= N+1. Depends on the parity of N itself. Still true no matter what: 2*Q > N

agreement is the principle of infectious disease transmission. In a complete graph,
At first, the first person was infected,
Every round, every infected person tries to randomly infect another person. If the other person has been infected, nothing happens. If the other person has not been infected, then the infected person will participate in each round. Randomly infect other people.

How long does it take to fully infect the entire graph. This is actually the same complexity if you need to try all the coupons. Expectation is O (N Log N)

